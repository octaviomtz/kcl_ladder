{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ladder network\n",
    "- Based on: https://github.com/abhiskk/ladder/blob/master/ladder/\n",
    "- An unsqueeze operation was added **bn_hat_z_layers**   \n",
    "\n",
    "## - Watch out:\n",
    "- Check if using the einsum is correct accurate\n",
    "- Check if the batch norm done in bn_hat_z_layers (decoder) is done in the correct axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "plt.rcParams.update({'font.size':18})\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_CLR\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](ladder_network_overview.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Algorithm overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### I. For each epoch:\n",
    "### 1. forward_noise (on labelled data)\n",
    "- create noise\n",
    "- create dirty data (h) $h = x + noise$\n",
    "- **store** the bottom h (h1) (without gradients)  $\\overset{\\sim}{z}$ (retrieved in 7)\n",
    "- For each encoder we send forward_noise(h):\n",
    "  - linear transformation: $y=xA^T$ (z_pre)\n",
    "  - Batch_Normalization (z_pre_norm)\n",
    "  - Noise addition $\\overset{\\sim}{z}$\n",
    "  - **Store** a copy of $\\overset{\\sim}{z}$ (retrieved in 3)\n",
    "  - Batch Norm correction\n",
    "  - Activation (h)\n",
    "  - Return (h)\n",
    "\n",
    "### 2. forward_noise (on unlabelled data)\n",
    "- same as 1\n",
    "\n",
    "### 3. get_encoders_tilde_z\n",
    " - Get all $\\overset{\\sim}{z}$ obtained in forward_noise\n",
    " - Append them in an array and reverse it\n",
    " \n",
    "### 4. forward_clean (unlabelled_data)\n",
    "- For each encoder we send forward_clean(h):\n",
    "  - linear transformation: $y=xA^T$ (z_pre)\n",
    "  - **Store** a copy of (**buffer_z_pre**) (retrieved in 5)\n",
    "  - Batch_Normalization (z)\n",
    "  - **Store** a copy of (**buffer_z**) (retrieved in 6)\n",
    "  - Batch_Normalization Gamm_Beta (z_gb)   <-check why\n",
    "  - Activation (h)\n",
    "  - Return (h)\n",
    "  \n",
    "### 5. get_encoders_z_pre (created in 4)\n",
    " - Get all **buffer_z_pre** obtained in forward_noise\n",
    " - Append them in an array and reverse it\n",
    " \n",
    "### 6. get_encoders_z (created in 4)\n",
    " - Get all **buffer_z** obtained in forward_noise\n",
    " - Append them in an array and reverse it\n",
    " \n",
    "### 7. buffer_tilde_z_bottom (created in 4)\n",
    "-  **Store** the bottom $\\overset{\\sim}{z}$ created in 1\n",
    "\n",
    "### 8. forward_decoders( $\\overset{\\sim}{z}$_unlabelled (from 3),    output_noise_unlabelled (from 2), bottom $\\overset{\\sim}{z}$_unlabelled (from 2))\n",
    "### - returns $\\hat{z}$ (weighted ($\\upsilon$) sum of $\\overset{\\sim}{z}$ and a prior $\\mu$) \n",
    "### $\\hat{z} = \\upsilon * \\overset{\\sim}{z} + (1 - \\upsilon) * \\mu$\n",
    "\n",
    "- Batch Normalization on encoder_output  \n",
    "- for each decoder:\n",
    "  - get the corresponding $\\overset{\\sim}{z}$ from $\\boldsymbol{\\overset{\\sim}{z}}$ (tilde_z from 3)\n",
    "  - apply u=decoder.forward($\\overset{\\sim}{z}$, u) (in the first case u is encoder_output)\n",
    "    - apply the combinatior function g(tilde_z, u). It combines the lateral noisy activation signal $\\overset{\\sim}{z}$ and the reconstruction from layer $\\hat{z}^{(l+1)}$\n",
    "    - In the first case the combinator will use the last $\\overset{\\sim}{z}$ and the encoder_output\n",
    "    - In the combinator the functions $\\mu$ and $\\upsilon$ are model as expresive non linearities and have trainable parameters ($a1_i$...$a5_i$). e.g:\n",
    "    - $\\mu$ = a1 \\* sigmoid(a2 \\* output + a3) + a4 \\* output + a5\n",
    "    - $\\upsilon$ = a6 \\* sigmoid(a7 \\* output + a8) + a9 \\* output + a10\n",
    "    - $\\mu$ and $\\upsilon$ have the same shape as u (output for the first case)\n",
    "    - $\\hat{z}$ = ($\\overset{\\sim}{z} - \\mu) * \\upsilon) + \\mu$    (equivalent to first formula in (8))\n",
    "    \n",
    "### 9-10. Append unlabelled to preactivations and to activations\n",
    "### 11. Batch norm $\\hat{z}$ using z_pre  \n",
    "  - It uses mean(z_pre) but the std is obtained with random noise (?) \n",
    "  - Warning: I added an unsqueeze operation to match mm dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Conv(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out, activation_type,\n",
    "                 train_bn_scaling, noise_level, use_cuda):\n",
    "        super(Encoder_Conv, self).__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.activation_type = activation_type\n",
    "        self.train_bn_scaling = train_bn_scaling\n",
    "        self.noise_level = noise_level\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # Encoder\n",
    "        # Encoder only uses W matrix, no bias\n",
    "        #self.linear = torch.nn.Linear(d_in, d_out, bias=False)\n",
    "        #self.linear.weight.data = torch.randn(self.linear.weight.data.size()) / np.sqrt(d_in)\n",
    "        self.conv = torch.nn.Conv2d(d_in, d_out, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.conv.weight.data = torch.randn(self.conv.weight.data.size()) / np.sqrt(d_in)\n",
    "\n",
    "        # Batch Normalization\n",
    "        # For Relu Beta of batch-norm is redundant, hence only Gamma is trained\n",
    "        # For Softmax Beta, Gamma are trained\n",
    "        # batch-normalization bias\n",
    "        self.bn_normalize_clean = torch.nn.BatchNorm2d(d_out, affine=False)\n",
    "        self.bn_normalize = torch.nn.BatchNorm2d(d_out, affine=False)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Activation\n",
    "        if activation_type == 'relu':\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        elif activation_type == 'softmax':\n",
    "            self.activation = torch.nn.Softmax()\n",
    "        elif activation_type == 'AdaptiveAvgPool':\n",
    "            self.activation = torch.nn.AdaptiveAvgPool2d((1))\n",
    "        else:\n",
    "            raise ValueError(\"invalid Activation type\")\n",
    "\n",
    "        # buffer for z_pre, z which will be used in decoder cost\n",
    "        self.buffer_z_pre = None\n",
    "        self.buffer_z = None\n",
    "        # buffer for tilde_z which will be used by decoder for reconstruction\n",
    "        self.buffer_tilde_z = None\n",
    "        \n",
    "        #Get shapes after convolution to use them later in the decoder\n",
    "        self.conv_shapes = None\n",
    "\n",
    "    def bn_gamma_beta(self, x, d_out, printout = False):\n",
    "        if self.use_cuda:\n",
    "            ones = Parameter(torch.ones(x.size()[0], x.size()[-1], x.size()[-1], 1).cuda())\n",
    "        else:\n",
    "            ones = Parameter(torch.ones(x.size()[0], x.size()[-1], x.size()[-1], 1))\n",
    "            \n",
    "        if self.use_cuda:\n",
    "            self.bn_beta = Parameter(torch.cuda.FloatTensor(1, d_out, x.size()[-1], x.size()[-1]))\n",
    "        else:\n",
    "            self.bn_beta = Parameter(torch.FloatTensor(1, d_out, x.size()[-1], x.size()[-1]))\n",
    "        self.bn_beta.data.zero_()\n",
    "        #t = x + ones.mm(self.bn_beta)\n",
    "        mult_out = torch.einsum('bhwi,iohw->bohw', (ones, self.bn_beta)) #batch_height_width_in,in_out_height_width\n",
    "        if printout: print(f'  {np.shape(self.bn_beta.cpu().detach().numpy())} = bn_beta')\n",
    "        t = x + mult_out\n",
    "        \n",
    "        if self.train_bn_scaling:\n",
    "            # batch-normalization scaling\n",
    "            if self.use_cuda:\n",
    "                self.bn_gamma = Parameter(torch.cuda.FloatTensor(1, d_out, x.size()[-1], x.size()[-1]))\n",
    "                self.bn_gamma.data = torch.ones(self.bn_gamma.size()).cuda()\n",
    "            else:\n",
    "                self.bn_gamma = Parameter(torch.FloatTensor(1, d_out, x.size()[-1], x.size()[-1]))\n",
    "                self.bn_gamma.data = torch.ones(self.bn_gamma.size())\n",
    "        \n",
    "        if self.train_bn_scaling:\n",
    "            #print(f'ones = {np.shape(ones)}, bn_gamma = {np.shape(self.bn_gamma)}')\n",
    "            mult_out = torch.einsum('bhwl,lchw->bchw', (ones, self.bn_gamma))\n",
    "            if printout: print(f'  {np.shape(self.bn_gamma.cpu().detach().numpy())} = bn_gamma')\n",
    "            t = torch.mul(t, mult_out)\n",
    "        return t\n",
    "\n",
    "    def forward_clean(self, h, printout = False):\n",
    "        if printout:  print(f'{np.shape(h.cpu().detach().numpy())} = h')\n",
    "        z_pre = self.conv(h) #MOD v0.3.0 \n",
    "        #Store z_pre, z to be used in calculation of reconstruction cost\n",
    "        self.buffer_z_pre = z_pre.detach().clone()\n",
    "        if printout:  print('conv2d')\n",
    "        if printout:  print(f'{np.shape(z_pre.cpu().detach().numpy())} = z_pre')\n",
    "        if printout:  print(f'{list(np.shape(z_pre.cpu().detach().numpy()))[-1]} = shape')\n",
    "        self.conv_shapes = list(np.shape(z_pre.cpu().detach().numpy()))[-1]\n",
    "        z = self.bn_normalize_clean(z_pre)\n",
    "        if printout:  print('BN')\n",
    "        self.buffer_z = z.detach().clone()\n",
    "        z_gb = self.bn_gamma_beta(z, self.d_out, printout= printout) #MOD v0.3.0\n",
    "        if str(self.activation) == 'Softmax()':\n",
    "            z_gb = z_gb.view(-1, z_gb.size(1))\n",
    "        if printout:  print(f'{str(self.activation)}')\n",
    "        h = self.activation(z_gb)\n",
    "        if printout:print(f'{np.shape(h.cpu().detach().numpy())} = h')\n",
    "        return h\n",
    "\n",
    "    def forward_noise(self, tilde_h, printout = False):\n",
    "        # z_pre will be used in the decoder cost\n",
    "        if printout: print(f'{np.shape(tilde_h.cpu().detach().numpy())} = tilde_h')\n",
    "        z_pre = self.conv(tilde_h) #MOD v0.3.0\n",
    "        if printout: print('conv2d')\n",
    "        if printout: print(f'{np.shape(z_pre.cpu().detach().numpy())} = z_pre')\n",
    "        z_pre_norm = self.bn_normalize(z_pre)\n",
    "        if printout: print('BN')\n",
    "        # Add noise\n",
    "        noise = np.random.normal(loc=0.0, scale=self.noise_level, size=z_pre_norm.size())\n",
    "        if self.use_cuda:\n",
    "            noise = Variable(torch.cuda.FloatTensor(noise))\n",
    "        else:\n",
    "            noise = Variable(torch.FloatTensor(noise))\n",
    "        if printout: print('add noise')\n",
    "        # tilde_z will be used by decoder for reconstruction\n",
    "        tilde_z = z_pre_norm + noise\n",
    "        # store tilde_z in buffer\n",
    "        self.buffer_tilde_z = tilde_z\n",
    "        if printout: print(f'{np.shape(tilde_z.cpu().detach().numpy())} = tilde_z')\n",
    "        if printout: print('BN correction')\n",
    "        z = self.bn_gamma_beta(tilde_z, self.d_out, printout=False) #MOD v0.3.0\n",
    "        if str(self.activation) == 'Softmax()':\n",
    "            z = z.view(-1, z.size(1))\n",
    "        if printout: print(f'{str(self.activation)}')\n",
    "        h = self.activation(z)\n",
    "        if printout: print(f'{np.shape(h.cpu().detach().numpy())} = h')\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedEncoders_Conv(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_encoders, activation_types,\n",
    "                 train_batch_norms, noise_std, use_cuda):\n",
    "        super(StackedEncoders_Conv, self).__init__()\n",
    "        self.buffer_tilde_z_bottom = None\n",
    "        self.encoders_ref = []\n",
    "        self.encoders = torch.nn.Sequential()\n",
    "        self.noise_level = noise_std\n",
    "        self.use_cuda = use_cuda\n",
    "        n_encoders = len(d_encoders)\n",
    "        \n",
    "        for i in range(n_encoders):\n",
    "            if i == 0:\n",
    "                d_input = d_in\n",
    "            else:\n",
    "                d_input = d_encoders[i - 1]\n",
    "            d_output = d_encoders[i]\n",
    "            activation = activation_types[i]\n",
    "            train_batch_norm = train_batch_norms[i]\n",
    "            encoder_ref = \"encoder_\" + str(i)\n",
    "            encoder = Encoder_Conv(d_input, d_output, activation, train_batch_norm, noise_std, use_cuda)\n",
    "            self.encoders_ref.append(encoder_ref)\n",
    "            self.encoders.add_module(encoder_ref, encoder)\n",
    "\n",
    "    def forward_clean(self, x, printout = False):\n",
    "        h = x\n",
    "        for e_ref in self.encoders_ref:\n",
    "            encoder = getattr(self.encoders, e_ref)\n",
    "            if printout: print(f'\\n{str(e_ref)}')\n",
    "            h = encoder.forward_clean(h, printout)\n",
    "        return h\n",
    "\n",
    "    def forward_noise(self, x, printout = False):\n",
    "        noise = np.random.normal(loc=0.0, scale=self.noise_level, size=x.size())\n",
    "        if self.use_cuda:\n",
    "            \n",
    "            noise = Variable(torch.cuda.FloatTensor(noise))\n",
    "            #noise = Variable(torch.FloatTensor(noise)).cuda()\n",
    "        else:\n",
    "            noise = Variable(torch.FloatTensor(noise))\n",
    "        h = x + noise\n",
    "        self.buffer_tilde_z_bottom = h.clone()\n",
    "        # pass through encoders\n",
    "        for e_ref in self.encoders_ref:\n",
    "            encoder = getattr(self.encoders, e_ref)\n",
    "            if printout: print(f'\\n{str(e_ref)}')\n",
    "            h = encoder.forward_noise(h, printout)\n",
    "        \n",
    "        return h\n",
    "\n",
    "    def get_encoders_tilde_z(self, reverse=True, printout = False):\n",
    "        tilde_z_layers = []\n",
    "        for e_ref in self.encoders_ref:\n",
    "            encoder = getattr(self.encoders, e_ref)\n",
    "            tilde_z = encoder.buffer_tilde_z.clone()\n",
    "            tilde_z_layers.append(tilde_z)\n",
    "        if reverse:\n",
    "            tilde_z_layers.reverse()\n",
    "        if printout: [print(f'tilde_z = {np.shape(i.cpu().detach().numpy())}') for i in tilde_z_layers]\n",
    "        return tilde_z_layers\n",
    "\n",
    "    def get_encoders_z_pre(self, reverse=True, printout = False):\n",
    "        z_pre_layers = []\n",
    "        for e_ref in self.encoders_ref:\n",
    "            encoder = getattr(self.encoders, e_ref)\n",
    "            z_pre = encoder.buffer_z_pre.clone()\n",
    "            z_pre_layers.append(z_pre)\n",
    "        if reverse:\n",
    "            z_pre_layers.reverse()\n",
    "        if printout: [print(f'z_pre_layers = {np.shape(i.cpu().detach().numpy())}') for i in z_pre_layers]\n",
    "        return z_pre_layers\n",
    "\n",
    "    def get_encoders_z(self, reverse=True, printout = False):\n",
    "        z_layers = []\n",
    "        for e_ref in self.encoders_ref:\n",
    "            encoder = getattr(self.encoders, e_ref)\n",
    "            z = encoder.buffer_z.clone()\n",
    "            z_layers.append(z)\n",
    "        if reverse:\n",
    "            z_layers.reverse()\n",
    "        if printout: [print(f'z_layers = {np.shape(i.cpu().detach().numpy())}') for i in z_layers]\n",
    "        return z_layers\n",
    "    \n",
    "    def get_shapes_after_conv(self, reverse = True, printout = False):\n",
    "        conv_shapes_layers = []\n",
    "        for e_ref in self.encoders_ref:\n",
    "            encoder = getattr(self.encoders, e_ref)\n",
    "            conv_shapes = encoder.conv_shapes\n",
    "            conv_shapes_layers.append(conv_shapes)\n",
    "        if printout: [print(i) for i in conv_shapes_layers]\n",
    "        if reverse:\n",
    "            conv_shapes_layers.reverse()\n",
    "        return conv_shapes_layers\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "### #1 Forward noise labelled\n",
    "**encoder_0**   \n",
    "(99, 1, 28, 28) = $\\tilde{h}$   \n",
    "conv2d   \n",
    "(99, 100, 14, 14) = z_pre   \n",
    "BN   \n",
    "add noise   \n",
    "(99, 100, 14, 14) = $\\tilde{z}$   \n",
    "BN correction   \n",
    "  (1, 100, 14, 14) = bn_beta   \n",
    "ReLU()   \n",
    "(99, 100, 14, 14) = h   \n",
    "\n",
    "**encoder_1**   \n",
    "(99, 100, 14, 14) = $\\tilde{h}$   \n",
    "conv2d   \n",
    "(99, 50, 7, 7) = z_pre   \n",
    "BN   \n",
    "add noise   \n",
    "(99, 50, 7, 7) = $\\tilde{z}$   \n",
    "BN correction   \n",
    "  (1, 50, 7, 7) = bn_beta   \n",
    "ReLU()   \n",
    "(99, 50, 7, 7) = h   \n",
    "\n",
    "**encoder_2**   \n",
    "(99, 50, 7, 7) = $\\tilde{h}$   \n",
    "conv2d   \n",
    "(99, 25, 4, 4) = z_pre   \n",
    "BN   \n",
    "add noise   \n",
    "(99, 25, 4, 4) = $\\tilde{z}$   \n",
    "BN correction   \n",
    "  (1, 25, 4, 4) = bn_beta   \n",
    "ReLU()   \n",
    "(99, 25, 4, 4) = h   \n",
    "\n",
    "**encoder_3**   \n",
    "(99, 25, 4, 4) = $\\tilde{h}$   \n",
    "conv2d   \n",
    "(99, 25, 2, 2) = z_pre   \n",
    "BN   \n",
    "add noise   \n",
    "(99, 25, 2, 2) = $\\tilde{z}$   \n",
    "BN correction   \n",
    "  (1, 25, 2, 2) = bn_beta   \n",
    "ReLU()   \n",
    "(99, 25, 2, 2) = h   \n",
    "\n",
    "**encoder_4**   \n",
    "(99, 25, 2, 2) = $\\tilde{h}$   \n",
    "conv2d   \n",
    "(99, 10, 1, 1) = z_pre   \n",
    "BN   \n",
    "add noise   \n",
    "(99, 10, 1, 1) = $\\tilde{z}$   \n",
    "BN correction   \n",
    "  (1, 10, 1, 1) = bn_beta   \n",
    "  (1, 10, 1, 1) = bn_gamma softmax   \n",
    "Softmax()   \n",
    "(99, 10) = h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "### #2 Forward noise unlabelled\n",
    "encoder_0   \n",
    "(143, 1, 28, 28) = $\\tilde{h}$   \n",
    "conv2d   \n",
    "(143, 100, 14, 14) = z_pre   \n",
    "BN   \n",
    "add noise   \n",
    "(143, 100, 14, 14) = $\\tilde{z}$   \n",
    "BN correction   \n",
    "  (1, 100, 14, 14) = bn_beta   \n",
    "ReLU()   \n",
    "(143, 100, 14, 14) = h   \n",
    "\n",
    "encoder_1   \n",
    "(143, 100, 14, 14) = $\\tilde{h}$   \n",
    "conv2d   \n",
    "(143, 50, 7, 7) = z_pre   \n",
    "BN   \n",
    "add noise   \n",
    "(143, 50, 7, 7) = $\\tilde{z}$   \n",
    "BN correction   \n",
    "  (1, 50, 7, 7) = bn_beta   \n",
    "ReLU()   \n",
    "(143, 50, 7, 7) = h   \n",
    "\n",
    "encoder_2   \n",
    "(143, 50, 7, 7) = $\\tilde{h}$   \n",
    "conv2d   \n",
    "(143, 25, 4, 4) = z_pre   \n",
    "BN   \n",
    "add noise   \n",
    "(143, 25, 4, 4) = $\\tilde{z}$   \n",
    "BN correction   \n",
    "  (1, 25, 4, 4) = bn_beta   \n",
    "ReLU()   \n",
    "(143, 25, 4, 4) = h   \n",
    "\n",
    "encoder_3   \n",
    "(143, 25, 4, 4) = $\\tilde{h}$   \n",
    "conv2d   \n",
    "(143, 25, 2, 2) = z_pre   \n",
    "BN   \n",
    "add noise   \n",
    "(143, 25, 2, 2) = $\\tilde{z}$   \n",
    "BN correction   \n",
    "  (1, 25, 2, 2) = bn_beta   \n",
    "ReLU()   \n",
    "(143, 25, 2, 2) = h   \n",
    "\n",
    "encoder_4   \n",
    "(143, 25, 2, 2) = $\\tilde{h}$   \n",
    "conv2d   \n",
    "(143, 10, 1, 1) = z_pre   \n",
    "BN   \n",
    "add noise   \n",
    "(143, 10, 1, 1) = $\\tilde{z}$   \n",
    "BN correction   \n",
    "  (1, 10, 1, 1) = bn_beta   \n",
    "  (1, 10, 1, 1) = bn_gamma   \n",
    "Softmax()   \n",
    "(143, 10) = h   \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "### #3 get $\\tilde{z}$  (from noise encoder)\n",
    "$\\tilde{z} $ = (143, 10, 1, 1)   \n",
    "$\\tilde{z} $ = (143, 25, 2, 2)   \n",
    "$\\tilde{z} $ = (143, 25, 4, 4)   \n",
    "$\\tilde{z} $ = (143, 50, 7, 7)   \n",
    "$\\tilde{z} $ = (143, 100, 14, 14)   \n",
    "   \n",
    "### #5 get ${z}$ pre BN   (from clean encoder)\n",
    "z_pre_layers = (143, 10, 1, 1)   \n",
    "z_pre_layers = (143, 25, 2, 2)   \n",
    "z_pre_layers = (143, 25, 4, 4)   \n",
    "z_pre_layers = (143, 50, 7, 7)   \n",
    "z_pre_layers = (143, 100, 14, 14)   \n",
    "\n",
    "### #6 get ${z}$ after BN   (from clean encoder)   \n",
    "z_layers = (143, 10, 1, 1)   \n",
    "z_layers = (143, 25, 2, 2)   \n",
    "z_layers = (143, 25, 4, 4)   \n",
    "z_layers = (143, 50, 7, 7)   \n",
    "z_layers = (143, 100, 14, 14)   \n",
    "\n",
    "### #7 get bottom $\\tilde{z}$ (from noise)\n",
    "z_bottom = (143, 1, 28, 28)\n",
    "\n",
    "---\n",
    "### For Decoder we need:   \n",
    "#3 $\\tilde{z}$  \n",
    "#7 bottom $\\tilde{z}$   \n",
    "output_noise_unlabelled = (143, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "### #4 Forward clean unlabelled\n",
    "encoder_0   \n",
    "(143, 1, 28, 28) = h   \n",
    "conv2d   \n",
    "(143, 100, 14, 14) = z_pre   \n",
    "BN   \n",
    "  (1, 100, 14, 14) = bn_beta   \n",
    "ReLU()   \n",
    "(143, 100, 14, 14) = h   \n",
    "\n",
    "encoder_1   \n",
    "(143, 100, 14, 14) = h   \n",
    "conv2d   \n",
    "(143, 50, 7, 7) = z_pre   \n",
    "BN   \n",
    "  (1, 50, 7, 7) = bn_beta   \n",
    "ReLU()   \n",
    "(143, 50, 7, 7) = h   \n",
    "\n",
    "encoder_2   \n",
    "(143, 50, 7, 7) = h   \n",
    "conv2d   \n",
    "(143, 25, 4, 4) = z_pre   \n",
    "BN   \n",
    "  (1, 25, 4, 4) = bn_beta   \n",
    "ReLU()   \n",
    "(143, 25, 4, 4) = h   \n",
    "\n",
    "encoder_3   \n",
    "(143, 25, 4, 4) = h   \n",
    "conv2d   \n",
    "(143, 25, 2, 2) = z_pre   \n",
    "BN   \n",
    "  (1, 25, 2, 2) = bn_beta   \n",
    "ReLU()   \n",
    "(143, 25, 2, 2) = h   \n",
    "\n",
    "encoder_4   \n",
    "(143, 25, 2, 2) = h   \n",
    "conv2d   \n",
    "(143, 10, 1, 1) = z_pre   \n",
    "BN   \n",
    "  (1, 10, 1, 1) = bn_beta   \n",
    "  (1, 10, 1, 1) = bn_gamma   \n",
    "Softmax()   \n",
    "(143, 10) = h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "### Decoder outputs\n",
    "### #8 hat_z_layers_unlabelled   \n",
    "$\\hat{z} = (143, 10, 1, 1)$   \n",
    "$\\hat{z} = (143, 25, 2, 2)$   \n",
    "$\\hat{z} = (143, 25, 4, 4)$   \n",
    "$\\hat{z} = (143, 50, 7, 7)$   \n",
    "$\\hat{z} = (143, 100, 14, 14)$   \n",
    "$\\hat{z} = (143, 1, 28, 28)$   \n",
    "\n",
    "### #9 z_pre_layers_unlabelled    \n",
    "z_pre = (143, 10, 1, 1)   \n",
    "z_pre = (143, 25, 2, 2)   \n",
    "z_pre = (143, 25, 4, 4)   \n",
    "z_pre = (143, 50, 7, 7)   \n",
    "z_pre = (143, 100, 14, 14)   \n",
    "z_pre = (143, 1, 28, 28)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "### #10 z_layers_unlabelled    \n",
    "z = (143, 10, 1, 1)   \n",
    "z = (143, 25, 2, 2)   \n",
    "z = (143, 25, 4, 4)   \n",
    "z = (143, 50, 7, 7)   \n",
    "z = (143, 100, 14, 14)   \n",
    "z = (143, 1, 28, 28)    \n",
    "\n",
    "### #11.bn_hat_z_layers_unlabelled\n",
    "bn $\\hat{z} = (143, 10, 1, 1)$   \n",
    "bn $\\hat{z} = (143, 25, 2, 2)$   \n",
    "bn $\\hat{z} = (143, 25, 4, 4)$   \n",
    "bn $\\hat{z} = (143, 50, 7, 7)$   \n",
    "bn $\\hat{z} = (143, 100, 14, 14)$   \n",
    "bn $\\hat{z} = (143, 1, 28, 28)$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out, tensor_shape, use_cuda):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        if self.use_cuda:\n",
    "            self.a1 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape).cuda())\n",
    "            self.a2 = Parameter(1. * torch.ones(1, d_in, tensor_shape, tensor_shape).cuda())\n",
    "            self.a3 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape).cuda())\n",
    "            self.a4 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape).cuda())\n",
    "            self.a5 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape).cuda())\n",
    "\n",
    "            self.a6 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape).cuda())\n",
    "            self.a7 = Parameter(1. * torch.ones(1, d_in, tensor_shape, tensor_shape).cuda())\n",
    "            self.a8 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape).cuda())\n",
    "            self.a9 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape).cuda())\n",
    "            self.a10 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape).cuda())\n",
    "        else:\n",
    "            self.a1 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape))\n",
    "            self.a2 = Parameter(1. * torch.ones(1, d_in, tensor_shape, tensor_shape))\n",
    "            self.a3 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape))\n",
    "            self.a4 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape))\n",
    "            self.a5 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape))\n",
    "\n",
    "            self.a6 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape))\n",
    "            self.a7 = Parameter(1. * torch.ones(1, d_in, tensor_shape, tensor_shape))\n",
    "            self.a8 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape))\n",
    "            self.a9 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape))\n",
    "            self.a10 = Parameter(0. * torch.ones(1, d_in, tensor_shape, tensor_shape))\n",
    "\n",
    "\n",
    "        if self.d_out is not None:\n",
    "            self.V = torch.nn.ConvTranspose2d(d_in, d_out, kernel_size = 3, stride = 2, padding=1, bias=False)\n",
    "            self.V.weight.data = torch.randn(self.V.weight.data.size()) / np.sqrt(d_in)\n",
    "            # batch-normalization for u\n",
    "            self.bn_normalize = torch.nn.BatchNorm2d(d_out, affine=False)\n",
    "\n",
    "        # buffer for hat_z_l to be used for cost calculation\n",
    "        self.buffer_hat_z_l = None\n",
    "\n",
    "    def g(self, tilde_z_l, u_l, printout):\n",
    "        if self.use_cuda:\n",
    "            ones = Parameter(torch.ones(tilde_z_l.size()[0], 1).cuda())\n",
    "        else:\n",
    "            ones = Parameter(torch.ones(tilde_z_l.size()[0], 1))\n",
    "        if printout: print(f'ones = {np.shape(ones)}, tilde_z = {np.shape(tilde_z_l)}') \n",
    "        if printout: print(f'a1 = {np.shape(self.a1.detach().cpu().numpy())}, a2 = {np.shape(self.a2.detach().cpu().numpy())}, \\\n",
    "a3 = {np.shape(self.a3.detach().cpu().numpy())}, a4 = {np.shape(self.a4.detach().cpu().numpy())}, a5 = {np.shape(self.a5.detach().cpu().numpy())}')\n",
    "        if printout: print(f'a6 = {np.shape(self.a6.detach().cpu().numpy())}, a7 = {np.shape(self.a7.detach().cpu().numpy())}, \\\n",
    "a8 = {np.shape(self.a8.detach().cpu().numpy())}, a9 = {np.shape(self.a9.detach().cpu().numpy())}, a10 = {np.shape(self.a10.detach().cpu().numpy())}')\n",
    "        \n",
    "        b_a1 = torch.einsum('cghw,bc->bghw', (self.a1, ones)) #channels_groups(classes)_height_width, batch_channels\n",
    "        b_a2 = torch.einsum('cghw,bc->bghw', (self.a2, ones)) \n",
    "        b_a3 = torch.einsum('cghw,bc->bghw', (self.a3, ones)) \n",
    "        b_a4 = torch.einsum('cghw,bc->bghw', (self.a4, ones)) \n",
    "        b_a5 = torch.einsum('cghw,bc->bghw', (self.a5, ones)) \n",
    "\n",
    "        b_a6 = torch.einsum('cghw,bc->bghw', (self.a6, ones)) \n",
    "        b_a7 = torch.einsum('cghw,bc->bghw', (self.a7, ones)) \n",
    "        b_a8 = torch.einsum('cghw,bc->bghw', (self.a8, ones)) \n",
    "        b_a9 = torch.einsum('cghw,bc->bghw', (self.a9, ones)) \n",
    "        b_a10 = torch.einsum('cghw,bc->bghw', (self.a10, ones)) \n",
    "        \n",
    "        if printout: print(f'b_a1 = {np.shape(b_a1.detach().cpu().numpy())}, b_a2 = {np.shape(b_a2.detach().cpu().numpy())}, \\\n",
    "b_a3 = {np.shape(b_a3.detach().cpu().numpy())}, b_a4 = {np.shape(b_a4.detach().cpu().numpy())}')\n",
    "        if printout: print(f'b_a5 = {np.shape(b_a5.detach().cpu().numpy())}, b_a6 = {np.shape(b_a6.detach().cpu().numpy())}, b_a7 = {np.shape(b_a7.detach().cpu().numpy())}, \\\n",
    "b_a8 = {np.shape(b_a8.detach().cpu().numpy())}')\n",
    "        if printout: print(f'b_a9 = {np.shape(b_a9.detach().cpu().numpy())}, b_a10 = {np.shape(b_a10.detach().cpu().numpy())}')\n",
    "\n",
    "        if printout: print(f'u_l = {np.shape(u_l.detach().cpu().numpy())}')\n",
    "        if printout: print(f'torch.mul(b_a2, u_l) = {np.shape(torch.mul(b_a2, u_l).detach().cpu().numpy())}')\n",
    "        \n",
    "        mu_l = torch.mul(b_a1, torch.sigmoid(torch.mul(b_a2, u_l) + b_a3)) + \\\n",
    "               torch.mul(b_a4, u_l) + \\\n",
    "               b_a5\n",
    "\n",
    "        v_l = torch.mul(b_a6, torch.sigmoid(torch.mul(b_a7, u_l) + b_a8)) + \\\n",
    "              torch.mul(b_a9, u_l) + \\\n",
    "              b_a10\n",
    "\n",
    "        hat_z_l = torch.mul(tilde_z_l - mu_l, v_l) + mu_l\n",
    "        \n",
    "        if printout: print(f'mu_l = {np.shape(mu_l)}, v_l = {np.shape(v_l)}')\n",
    "        return hat_z_l\n",
    "\n",
    "    def forward(self, tilde_z_l, u_l, tensor_shape, printout):\n",
    "        # hat_z_l will be used for calculating decoder costs\n",
    "        hat_z_l = self.g(tilde_z_l, u_l, printout)\n",
    "        # store hat_z_l in buffer for cost calculation\n",
    "        self.buffer_hat_z_l = hat_z_l\n",
    "        if printout: print(f'hat_z (before conv) = {np.shape(hat_z_l.detach().cpu().numpy())}')\n",
    "        if self.d_out is not None:\n",
    "            t = self.V(hat_z_l, output_size=(-1,-1,tensor_shape,tensor_shape))\n",
    "            if printout: print(f't (after conv) = {np.shape(t.detach().cpu().numpy())}')\n",
    "            u_l_below = self.bn_normalize(t)\n",
    "            \n",
    "#             t = self.V.forward(hat_z_l)\n",
    "#             print(f't (after conv) = {np.shape(t.detach().cpu().numpy())}')\n",
    "#             u_l_below = self.bn_normalize(t)\n",
    "            return u_l_below\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedDecoders_Conv(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_decoders, image_size, tensor_shapes, use_cuda):\n",
    "        super(StackedDecoders_Conv, self).__init__()\n",
    "        self.bn_u_top = torch.nn.BatchNorm1d(d_in, affine=False) \n",
    "        self.decoders_ref = []\n",
    "        self.decoders = torch.nn.Sequential()\n",
    "        self.use_cuda = use_cuda\n",
    "        n_decoders = len(d_decoders)\n",
    "        for i in range(n_decoders):\n",
    "            if i == 0:\n",
    "                d_input = d_in\n",
    "            else:\n",
    "                d_input = d_decoders[i - 1]\n",
    "            d_output = d_decoders[i]\n",
    "            tensor_shape = tensor_shapes[i]\n",
    "            decoder_ref = \"decoder_\" + str(i)\n",
    "            decoder = Decoder(d_input, d_output, tensor_shape, use_cuda)\n",
    "            self.decoders_ref.append(decoder_ref)\n",
    "            self.decoders.add_module(decoder_ref, decoder)\n",
    "\n",
    "        self.bottom_decoder = Decoder(image_size, None, 28, use_cuda)\n",
    "        self.tensor_shapes = tensor_shapes\n",
    "        \n",
    "    def forward(self, tilde_z_layers, u_top, tilde_z_bottom, printout = False):\n",
    "        # Note that tilde_z_layers should be in reversed order of encoders\n",
    "        hat_z = []\n",
    "        u = self.bn_u_top(u_top)\n",
    "        for i in range(len(self.decoders_ref)):\n",
    "            d_ref = self.decoders_ref[i]\n",
    "            decoder = getattr(self.decoders, d_ref)\n",
    "            tilde_z = tilde_z_layers[i]\n",
    "            if i == 0: \n",
    "                u.unsqueeze_(-1)\n",
    "                u.unsqueeze_(-1)\n",
    "            if printout: print(f'u before decoder = {np.shape(u.detach().cpu().numpy())}')\n",
    "            tensor_shape = self.tensor_shapes[i+1]\n",
    "            u = decoder.forward(tilde_z, u, tensor_shape, printout)\n",
    "            if printout: print(f'u after decoder = {np.shape(u.detach().cpu().numpy())}')\n",
    "            if printout: print('')\n",
    "            hat_z.append(decoder.buffer_hat_z_l)\n",
    "        self.bottom_decoder.forward(tilde_z_bottom, u, tensor_shapes[-1], printout)\n",
    "        hat_z_bottom = self.bottom_decoder.buffer_hat_z_l.clone()\n",
    "        hat_z.append(hat_z_bottom)\n",
    "        return hat_z\n",
    "\n",
    "    def bn_hat_z_layers(self, hat_z_layers, z_pre_layers, printout = False):\n",
    "        # TODO: Calculate batchnorm using GPU Tensors.\n",
    "        assert len(hat_z_layers) == len(z_pre_layers)\n",
    "        hat_z_layers_normalized = []\n",
    "        for i, (hat_z, z_pre) in enumerate(zip(hat_z_layers, z_pre_layers)):\n",
    "            tensor_shape = self.tensor_shapes[i]\n",
    "            if printout: print(f'hat_z = {np.shape(hat_z.detach().cpu().numpy())}, z_pre = {np.shape(z_pre.detach().cpu().numpy())}')\n",
    "            if self.use_cuda:\n",
    "                ones = Variable(torch.ones(z_pre.size()[0], 1, tensor_shape, tensor_shape).cuda())\n",
    "            else:\n",
    "                ones = Variable(torch.ones(z_pre.size()[0], 1, tensor_shape, tensor_shape))\n",
    "            if printout: print(f'ones = {np.shape(ones.detach().cpu().numpy())}')\n",
    "            mean = torch.mean(z_pre, 0)\n",
    "            mean.unsqueeze_(0) # <---- ADDED BY OMM\n",
    "            if printout: print(f'mean = {np.shape(mean.detach().cpu().numpy())}')\n",
    "            noise_var = np.random.normal(loc=0.0, scale=1 - 1e-10, size=z_pre.size())\n",
    "            if printout: print(f'z_pre = {np.shape(z_pre.detach().cpu().numpy())}')\n",
    "            if printout: print(f'noise_var = {np.shape(noise_var)}')\n",
    "            tensor_shape = self.tensor_shapes[i]\n",
    "            #pdb.set_trace()\n",
    "            if self.use_cuda: # add the tensor_shape into the reshape function\n",
    "                var = np.var(z_pre.data.cpu().numpy() + noise_var, axis=0).reshape(1, z_pre.size()[1], tensor_shape, tensor_shape)\n",
    "            else:\n",
    "                var = np.var(z_pre.data.numpy() + noise_var, axis=0).reshape(1, z_pre.size()[1], tensor_shape, tensor_shape)\n",
    "            var = Variable(torch.FloatTensor(var))\n",
    "            if printout: print(f'var = {np.shape(var.detach().cpu().numpy())}')\n",
    "            if self.use_cuda:\n",
    "                hat_z = hat_z.cpu()\n",
    "                ones = ones.cpu()\n",
    "                mean = mean.cpu()\n",
    "            onesmmmean = torch.einsum('bchw,cghw->bghw',(ones,mean)) # batch_channel_height_width, channels_groups(classes)_height_width\n",
    "            if printout: print(f'onesmmmean = {np.shape(onesmmmean.detach().cpu().numpy())}')\n",
    "            torchsqrt = torch.sqrt(var + 1e-10)\n",
    "            if printout: print(f'torchsqrt = {np.shape(torchsqrt.detach().cpu().numpy())}')\n",
    "            ones_torchsqrt = torch.einsum('bchw,vohw->bohw',(ones,torchsqrt)) #batch_channel_height_width, variance_channels\n",
    "            if printout: print(f'ones_torchsqrt = {np.shape(ones_torchsqrt.detach().cpu().numpy())}')\n",
    "            #hat_z_normalized = torch.div(hat_z - ones.mm(mean), ones.mm(torch.sqrt(var + 1e-10)))\n",
    "            hat_z_normalized = torch.div(hat_z - onesmmmean, ones_torchsqrt)\n",
    "            if self.use_cuda:\n",
    "                hat_z_normalized = hat_z_normalized.cuda()\n",
    "            hat_z_layers_normalized.append(hat_z_normalized)\n",
    "            print('')\n",
    "        return hat_z_layers_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# class Ladder(torch.nn.Module):\n",
    "#     def __init__(self, encoder_sizes, decoder_sizes, encoder_activations, encoder_train_bn_scaling,\n",
    "#                 noise_std, use_cuda):\n",
    "#         super(Ladder, self).__init__()\n",
    "#         self.use_cuda = use_cuda\n",
    "#         decoder_in = encoder_sizes[-1]\n",
    "#         encoder_in = decoder_sizes[-1]\n",
    "#         self.se = StackedEncoders(encoder_in, encoder_sizes, encoder_activations,\n",
    "#                                   encoder_train_bn_scaling, noise_std, use_cuda)\n",
    "#         self.de = StackedDecoders(decoder_in, decoder_sizes, encoder_in, use_cuda)\n",
    "        \n",
    "#         self.bn_image = torch.nn.BatchNorm1d(encoder_in, affine=False)\n",
    "        \n",
    "#     def forward_encoders_clean(self, data):\n",
    "#         return self.se.forward_clean(data)\n",
    "        \n",
    "#     def forward_encoders_noise(self, data):\n",
    "#         return self.se.forward_noise(data)\n",
    "    \n",
    "#     def forward_decoders(self, tilde_z_layers, encoder_output, tilde_z_bottom):\n",
    "#         return self.de.forward(tilde_z_layers, encoder_output, tilde_z_bottom)\n",
    "        \n",
    "#     def get_encoders_tilde_z(self, reverse=True):\n",
    "#         return self.se.get_encoders_tilde_z(reverse)\n",
    "\n",
    "#     def get_encoders_z_pre(self, reverse=True):\n",
    "#         return self.se.get_encoders_z_pre(reverse)\n",
    "\n",
    "#     def get_encoder_tilde_z_bottom(self):\n",
    "#         return self.se.buffer_tilde_z_bottom.clone()\n",
    "\n",
    "#     def get_encoders_z(self, reverse=True):\n",
    "#         return self.se.get_encoders_z(reverse)\n",
    "\n",
    "#     def decoder_bn_hat_z_layers(self, hat_z_layers, z_pre_layers):\n",
    "#         return self.de.bn_hat_z_layers(hat_z_layers, z_pre_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root='data/'\n",
    "train_set = dset.MNIST(root=root, train=True, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_labelled = torch.Size([1600, 1, 28, 28]), 10% of 16000\n",
      "X_train_unlabelled = torch.Size([14400, 1, 28, 28]), 90% of 16000\n",
      "labels = torch.Size([1600])\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST and permutate it\n",
    "X_train = train_set.train_data\n",
    "y_train = train_set.train_labels\n",
    "# Work with a subset of the samples\n",
    "X_train = X_train[:16000]\n",
    "y_train = y_train[:16000]\n",
    "# Add channel dimension\n",
    "X_train = X_train.view(-1,1,28,28)\n",
    "# Normalize it\n",
    "X_train = np.multiply(X_train, 1./255.)\n",
    "# Flatten the rows and columns\n",
    "# X_train = X_train.reshape(X_train.shape[0],X_train.shape[1]*X_train.shape[2])\n",
    "randomize = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(randomize)\n",
    "# Get a ratio from labeled and unlabeled data\n",
    "labeled_ratio = .1\n",
    "X_train_labelled = X_train[randomize[:int(np.round(X_train.shape[0] * labeled_ratio))]] \n",
    "y_train = y_train[randomize[:int(np.round(X_train.shape[0] * labeled_ratio))]]\n",
    "X_train_unlabelled = X_train[randomize[int(np.round(X_train.shape[0] * labeled_ratio)):]] \n",
    "# Covert to tensor\n",
    "X_train_labelled = X_train_labelled.type(torch.Tensor)\n",
    "X_train_unlabelled = X_train_unlabelled.type(torch.Tensor)\n",
    "#X_train = np.expand_dims(X_train,-1)\n",
    "print(f'X_train_labelled = {np.shape(X_train_labelled)}, {labeled_ratio*100:.0f}% of {X_train.shape[0]}')\n",
    "print(f'X_train_unlabelled = {np.shape(X_train_unlabelled)}, {(1-labeled_ratio)*100:.0f}% of {X_train.shape[0]}')\n",
    "print(f'labels = {np.shape(y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test data\n",
    "X_test = test_set.test_data\n",
    "y_test = test_set.test_labels\n",
    "X_test = np.multiply(X_test, 1./255)\n",
    "X_test = X_test.view(-1,1,28,28)\n",
    "#X_test = X_test.reshape(X_test.shape[0],X_test.shape[1]*X_test.shape[2])\n",
    "X_test = X_test.type(torch.Tensor)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "X_test = Variable(X_test, requires_grad = False)\n",
    "y_test = Variable(y_test, requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1,2)\n",
    "# ax[0].imshow(np.reshape(X_train[0],[28,28]))\n",
    "# ax[1].imshow(np.reshape(X_test[0],[28,28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # For debugging\n",
    "# X_train_labelled = X_train_labelled[:300]\n",
    "# X_train_unlabelled = X_train_unlabelled[:300]\n",
    "# y_train = y_train[:300]\n",
    "# X_test = X_test[:300]\n",
    "# y_test = y_test[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "details = '' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 4, 7, 14, 28]\n"
     ]
    }
   ],
   "source": [
    "tensor_shapes = []\n",
    "input_height = np.shape(X_train.detach().cpu().numpy())[-1]\n",
    "tensor_shapes.append(input_height)\n",
    "while input_height >1:\n",
    "    input_height/=2\n",
    "    input_height = np.ceil(input_height)\n",
    "    tensor_shapes.append(int(input_height))\n",
    "tensor_shapes.reverse()\n",
    "print(tensor_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11679974c99d4b5486505fccd51892cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/om18/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:124: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/om18/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:95: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noise_std = 0.2\n",
    "encoder_sizes = [100, 50, 25, 25, 10]\n",
    "encoder_activations =  ['relu','relu','relu','relu', 'softmax']\n",
    "encoder_train_bn_scaling = [False, False,False, False, True]\n",
    "use_cuda = True\n",
    "learning_rate = 0.02\n",
    "my_encoder = StackedEncoders_Conv(1, encoder_sizes, encoder_activations,\n",
    "                                  encoder_train_bn_scaling, noise_std, use_cuda)\n",
    "\n",
    "decoder_sizes = [25, 25, 50, 100, 1]\n",
    "unsupervised_costs_lambda = [0.1, 0.1, 10., 1000.]\n",
    "\n",
    "my_decoder = StackedDecoders_Conv(10, decoder_sizes, 1, tensor_shapes, use_cuda) \n",
    "\n",
    "if use_cuda:\n",
    "    my_encoder.cuda()\n",
    "    my_decoder.cuda()\n",
    "    \n",
    "optimizer = Adam(my_encoder.parameters(), lr = learning_rate)\n",
    "\n",
    "batch_size = 100\n",
    "batch_times = np.shape(X_train_labelled)[0]//batch_size\n",
    "batch_size_unlabelled = int(np.shape(X_train_unlabelled)[0] / batch_size) # v0.2.6\n",
    "\n",
    "loss = []\n",
    "accuracies = []\n",
    "y_pred_all = []\n",
    "for epoch in tqdm_notebook(range(1)):\n",
    "    \n",
    "    for i in range(batch_times):\n",
    "        if i>=1:continue # Uncomment to view tensor shapes\n",
    "                    \n",
    "        batch_train_labelled_images = torch.FloatTensor(X_train_labelled[i*batch_size:(i+1)*batch_size-1])\n",
    "        batch_train_unlabelled_images = torch.FloatTensor(X_train_unlabelled[i*batch_size_unlabelled:(i+1)*batch_size_unlabelled-1])\n",
    "        batch_train_labelled_labels = torch.LongTensor(y_train[i*batch_size:(i+1)*batch_size-1])\n",
    "        \n",
    "        if use_cuda:\n",
    "            batch_train_labelled_images = batch_train_labelled_images.cuda()\n",
    "            batch_train_unlabelled_images = batch_train_unlabelled_images.cuda()\n",
    "            batch_train_labelled_labels = batch_train_labelled_labels.cuda()\n",
    "        \n",
    "        labelled_data = Variable(batch_train_labelled_images, requires_grad=False)\n",
    "        unlabelled_data = Variable(batch_train_unlabelled_images, requires_grad=False)\n",
    "        labelled_target = Variable(batch_train_labelled_labels)\n",
    "\n",
    "        my_encoder.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #1 forward noise labelled\n",
    "        output_noise_labelled = my_encoder.forward_noise(labelled_data, printout = False)\n",
    "        \n",
    "        #2 forward noise unlabelled\n",
    "        output_noise_unlabelled = my_encoder.forward_noise(unlabelled_data, printout = False)        \n",
    "\n",
    "        #3 get_encoders_tilde_z\n",
    "        tilde_z_layers_unlabelled = my_encoder.get_encoders_tilde_z(reverse=True, printout = False)\n",
    "\n",
    "        #4 forward_encoders_clean\n",
    "        output_clean_unlabelled = my_encoder.forward_clean(unlabelled_data, printout = False)\n",
    "\n",
    "        #5 get_encoders_z_pre\n",
    "        z_pre_layers_unlabelled = my_encoder.get_encoders_z_pre(reverse=True, printout = False)\n",
    "\n",
    "        #6 get_encoders_z\n",
    "        z_layers_unlabelled = my_encoder.get_encoders_z(reverse=True, printout = False)\n",
    "\n",
    "        #7 get_encoder_tilde_z_bottom\n",
    "        tilde_z_bottom_unlabelled = my_encoder.buffer_tilde_z_bottom.clone()\n",
    "        \n",
    "        #Extra get tensor shapes after convolutions in encoder\n",
    "        tensor_shapes_after_convs = my_encoder.get_shapes_after_conv(printout = False)\n",
    "        \n",
    "        #DECODERS\n",
    "        #8 \n",
    "        hat_z_layers_unlabelled = my_decoder.forward(tilde_z_layers_unlabelled, \n",
    "                                                        output_noise_unlabelled,\n",
    "                                                        tilde_z_bottom_unlabelled, printout = False)\n",
    "        #9  \n",
    "        z_pre_layers_unlabelled.append(unlabelled_data)\n",
    "        #10\n",
    "        z_layers_unlabelled.append(unlabelled_data)\n",
    "        #11\n",
    "        bn_hat_z_layers_unlabelled = my_decoder.bn_hat_z_layers(hat_z_layers_unlabelled, \n",
    "                                                                   z_pre_layers_unlabelled, printout = False)\n",
    "\n",
    "        loss_supervised = torch.nn.CrossEntropyLoss()\n",
    "        cost = loss_supervised(output_noise_labelled, labelled_target)\n",
    "\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss.append(cost.item())\n",
    "    \n",
    "    my_encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        if use_cuda:\n",
    "            X_test = X_test.cuda()\n",
    "        \n",
    "        X_test_cuda = Variable(X_test)\n",
    "        y_pred = my_encoder.forward_clean(X_test_cuda)\n",
    "        y_pred = np.squeeze(y_pred.data.cpu().numpy())\n",
    "        y_pred_all.append(y_pred)\n",
    "    \n",
    "    pred = np.argmax(output_noise_labelled.detach().cpu().numpy(), axis=1)\n",
    "    accuracies.append(accuracy_score(labelled_target.cpu().numpy(), pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn_hat_z_layers_unlabelled = (143, 10, 1, 1)\n",
      "bn_hat_z_layers_unlabelled = (143, 25, 2, 2)\n",
      "bn_hat_z_layers_unlabelled = (143, 25, 4, 4)\n",
      "bn_hat_z_layers_unlabelled = (143, 50, 7, 7)\n",
      "bn_hat_z_layers_unlabelled = (143, 100, 14, 14)\n",
      "bn_hat_z_layers_unlabelled = (143, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "for i in bn_hat_z_layers_unlabelled: print(f'bn_hat_z_layers_unlabelled = {np.shape(i.detach().cpu().numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hat_z_layers_unlabelled: print(f'hat_z = {np.shape(i.detach().cpu().numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in z_pre_layers_unlabelled: print(f'z_pre = {np.shape(i.detach().cpu().numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in z_layers_unlabelled: print(f'z = {np.shape(i.detach().cpu().numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 4\n",
    "padding  = 1\n",
    "stride = 2\n",
    "a = torch.ones((143,1,1,1))\n",
    "print(a.shape)\n",
    "my_conv2d = torch.nn.ConvTranspose2d(1, 25, kernel_size = kernel_size, padding = padding, stride = stride)\n",
    "my_conv2d.output_size = (-1, -1, 2, 2)\n",
    "a = my_conv2d(a)\n",
    "print(a.shape)\n",
    "\n",
    "my_conv2d = torch.nn.ConvTranspose2d(25, 50, kernel_size = kernel_size, padding = padding, stride = stride)\n",
    "my_conv2d.output_size = (-1, -1, 4, 4)\n",
    "a = my_conv2d(a)\n",
    "print(a.shape)\n",
    "\n",
    "my_conv2d = torch.nn.ConvTranspose2d(50, 50, kernel_size = kernel_size, padding = padding, stride = stride)\n",
    "my_conv2d.output_size = (-1, -1, 7, 7)\n",
    "a = my_conv2d(a)\n",
    "print(a.shape)\n",
    "\n",
    "my_conv2d = torch.nn.ConvTranspose2d(50, 100, kernel_size = kernel_size, padding = padding, stride = stride)\n",
    "my_conv2d.output_size = (-1, -1, 14, 14)\n",
    "a = my_conv2d(a)\n",
    "print(a.shape)\n",
    "\n",
    "my_conv2d = torch.nn.ConvTranspose2d(100, 1, kernel_size = kernel_size, padding = padding, stride = stride)\n",
    "my_conv2d.output_size = (-1, -1, 28, 28)\n",
    "a = my_conv2d(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 4\n",
    "padding  = 1\n",
    "stride = 2\n",
    "a = torch.ones((143,1,1,1))\n",
    "print(a.shape)\n",
    "my_conv2d = torch.nn.ConvTranspose2d(1, 25, kernel_size = kernel_size, padding = padding, stride = stride)\n",
    "a = my_conv2d(a)\n",
    "print(a.shape)\n",
    "\n",
    "my_conv2d = torch.nn.ConvTranspose2d(25, 50, kernel_size = kernel_size, padding = padding, stride = stride)\n",
    "a = my_conv2d(a)\n",
    "print(a.shape)\n",
    "\n",
    "my_conv2d = torch.nn.ConvTranspose2d(50, 50, kernel_size = kernel_size, padding = padding, stride = stride)\n",
    "a = my_conv2d(a)\n",
    "print(a.shape)\n",
    "\n",
    "my_conv2d = torch.nn.ConvTranspose2d(50, 100, kernel_size = kernel_size, padding = padding, stride = stride)\n",
    "a = my_conv2d(a)\n",
    "print(a.shape)\n",
    "\n",
    "my_conv2d = torch.nn.ConvTranspose2d(100, 1, kernel_size = kernel_size, padding = padding, stride = stride)\n",
    "a = my_conv2d(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.ones((143, 10, 1, 1))*3\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=torch.ones((143, 10, 1, 1))*2\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.mul(a,b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mul(b_a2, u_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((1,10,1,1))\n",
    "b = np.ones((143,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.einsum('cChw,bc->bChw',a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_l = torch.mul(b_a1, torch.sigmoid(torch.mul(b_a2, u_l) + b_a3)) + \\\n",
    "               torch.mul(b_a4, u_l) + \\\n",
    "               b_a5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tensor_shapes_after_convs: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_test = []\n",
    "for i in y_pred_all:\n",
    "    #pred_proba = i.data.cpu()\n",
    "    pred = np.argmax(i, 1)\n",
    "    accuracies_test.append(accuracy_score(y_test.numpy(),pred))\n",
    "fig, ax = plt.subplots(1,2,figsize=(11,6))\n",
    "ax[0].plot(accuracies, label = 'train')\n",
    "ax[0].plot(accuracies_test, label = 'test')\n",
    "ax[0].legend()\n",
    "ax[1].plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test.cpu().numpy(), pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Configure the Ladder\n",
    "noise_std = 0.2\n",
    "encoder_sizes = [1000, 500, 250, 250, 250, 10]\n",
    "decoder_sizes = [250, 250, 250, 500, 1000, 784]\n",
    "unsupervised_costs_lambda = [0.1, 0.1, 0.1, 0.1, 0.1, 10., 1000.]  # 0.1, 0.1, 0.1, 0.1, 0.1, 10., 1000.\n",
    "encoder_activations = [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"softmax\"]\n",
    "encoder_train_bn_scaling = [False, False, False, False, False, True]\n",
    "\n",
    "use_cuda = True \n",
    "\n",
    "ladder = Ladder(encoder_sizes, decoder_sizes, encoder_activations,\n",
    "                    encoder_train_bn_scaling, noise_std, use_cuda)\n",
    "\n",
    "epochs = 50\n",
    "decay_epoch = 15\n",
    "learning_rate = 0.02\n",
    "initial_learning_rate = learning_rate\n",
    "optimizer = Adam(ladder.parameters(), lr = learning_rate)\n",
    "loss_supervised = torch.nn.CrossEntropyLoss()\n",
    "loss_unsupervised = torch.nn.MSELoss()\n",
    "\n",
    "if use_cuda:\n",
    "    ladder.cuda()\n",
    "\n",
    "batch_size = 100\n",
    "batch_times = np.shape(X_train_labelled)[0]//batch_size\n",
    "batch_size_unlabelled = int(np.shape(X_train_unlabelled)[0] / batch_size)\n",
    "\n",
    "supervised_costs = []\n",
    "unsupervised_costs = []\n",
    "cost_main = []\n",
    "y_pred_all = []\n",
    "\n",
    "agg_cost = 0.\n",
    "agg_supervised_cost = 0.\n",
    "agg_unsupervised_cost = 0.\n",
    "num_batches = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    if epoch > decay_epoch:\n",
    "        ratio = float(epochs - epoch) / (epochs - decay_epoch)\n",
    "        learning_rate = learning_rate * ratio\n",
    "        optimizer = Adam(ladder.parameters(), lr = learning_rate)\n",
    "        \n",
    "    for i in range(batch_times):\n",
    "        \n",
    "        if (i+1)%100==0 or i==batch_times-1: print(f'epoch = {epoch}, batch = {i+1}')\n",
    "    \n",
    "        ladder.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_train_labelled_images = torch.FloatTensor(X_train_labelled[i*batch_size:(i+1)*batch_size-1])\n",
    "        batch_train_unlabelled_images = torch.FloatTensor(X_train_unlabelled[i*batch_size_unlabelled:(i+1)*batch_size_unlabelled-1])\n",
    "        batch_train_labelled_labels = torch.LongTensor(y_train[i*batch_size:(i+1)*batch_size-1])\n",
    "        \n",
    "        if use_cuda:\n",
    "            batch_train_labelled_images = batch_train_labelled_images.cuda()\n",
    "            batch_train_unlabelled_images = batch_train_unlabelled_images.cuda()\n",
    "            batch_train_labelled_labels = batch_train_labelled_labels.cuda()\n",
    "        \n",
    "        labelled_data = Variable(batch_train_labelled_images, requires_grad=False)\n",
    "        unlabelled_data = Variable(batch_train_unlabelled_images, requires_grad=False)\n",
    "        labelled_target = Variable(batch_train_labelled_labels)\n",
    "        \n",
    "        # encoders\n",
    "        output_noise_labelled = ladder.forward_encoders_noise(labelled_data)            # 1\n",
    "        output_noise_unlabelled = ladder.forward_encoders_noise(unlabelled_data)        # 2\n",
    "        tilde_z_layers_unlabelled = ladder.get_encoders_tilde_z(reverse=True)           # 3\n",
    "        output_clean_unlabelled = ladder.forward_encoders_clean(unlabelled_data)        # 4\n",
    "        z_pre_layers_unlabelled = ladder.get_encoders_z_pre(reverse=True)               # 5\n",
    "        z_layers_unlabelled = ladder.get_encoders_z(reverse=True)                       # 6\n",
    "        tilde_z_bottom_unlabelled = ladder.get_encoder_tilde_z_bottom()                 # 7\n",
    "        # decoders\n",
    "        hat_z_layers_unlabelled = ladder.forward_decoders(tilde_z_layers_unlabelled,    \n",
    "                                                      output_noise_unlabelled,\n",
    "                                                      tilde_z_bottom_unlabelled)\n",
    "        z_pre_layers_unlabelled.append(unlabelled_data)\n",
    "        z_layers_unlabelled.append(unlabelled_data)\n",
    "        \n",
    "        bn_hat_z_layers_unlabelled = ladder.decoder_bn_hat_z_layers(hat_z_layers_unlabelled, \n",
    "                                                                z_pre_layers_unlabelled)\n",
    "        # costs\n",
    "        cost_supervised = loss_supervised.forward(output_noise_labelled, labelled_target)\n",
    "        cost_unsupervised = 0.\n",
    "        assert len(z_layers_unlabelled) == len(bn_hat_z_layers_unlabelled)\n",
    "        for cost_lambda, z, bn_hat_z in zip(unsupervised_costs_lambda, z_layers_unlabelled, bn_hat_z_layers_unlabelled):\n",
    "            c = cost_lambda * loss_unsupervised.forward(bn_hat_z, z)\n",
    "            cost_unsupervised += c\n",
    "    \n",
    "        # backprop\n",
    "        cost = cost_supervised + cost_unsupervised\n",
    "        cost.backward()                  \n",
    "        optimizer.step()\n",
    "    \n",
    "        agg_cost += cost.item()\n",
    "        agg_supervised_cost += cost_supervised.item()\n",
    "        agg_unsupervised_cost += cost_unsupervised.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        supervised_costs.append(cost_supervised.item())\n",
    "        unsupervised_costs.append(cost_unsupervised.item())\n",
    "        cost_main.append(cost.item())\n",
    "        \n",
    "    # eval\n",
    "    ladder.eval()\n",
    "    with torch.no_grad(): # So we don't run out of memory \n",
    "        if use_cuda:\n",
    "            X_test = X_test.cuda()\n",
    "        \n",
    "        X_test_cuda = Variable(X_test)\n",
    "        y_pred = ladder.forward_encoders_clean(X_test_cuda)\n",
    "        y_pred_all.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,9))\n",
    "accuracies = []\n",
    "for i in y_pred_all:\n",
    "    pred_proba = i.data.cpu()\n",
    "    pred = np.argmax(pred_proba, 1)\n",
    "    accuracies.append(accuracy_score(y_test.numpy(),pred))\n",
    "plt.plot(np.arange(1,len(accuracies)+1),accuracies)\n",
    "plt.ylabel('Accuracy', fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.title('Test Acc', fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "notes = mpatches.Patch(color='#1F77B4', label=f'cost.back(), \\nini_lr = {initial_learning_rate:.3f}, \\\n",
    "last_lamda={unsupervised_costs_lambda[-1]} \\ndecay_ep={decay_epoch}, \\nlabelled={np.shape(X_train_labelled)[0]}({labeled_ratio}%)\\\n",
    "\\nunlabelled={np.shape(X_train_unlabelled)[0]}({1-labeled_ratio}%)\\\n",
    "\\ntest = {np.shape(X_test)[0]}')\n",
    "plt.legend(handles=[notes], fontsize=18)\n",
    "plt.savefig(f'ladder_mnist_acc_{int(labeled_ratio*100)}%_label_{initial_learning_rate:.3f}_lr_{int(unsupervised_costs_lambda[-1])}_lamda_{details}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,9))\n",
    "name_cost_main, = plt.plot(np.arange(1,len(cost_main)+1), cost_main, label='main_cost')\n",
    "name_supervised_costs, = plt.plot(np.arange(1,len(supervised_costs)+1), supervised_costs, label='supervised_cost')\n",
    "name_unsupervised_costs, = plt.plot(np.arange(1,len(unsupervised_costs)+1), unsupervised_costs, label='unsupervised_cost')\n",
    "plt.ylabel('Loss', fontsize=18)\n",
    "plt.xlabel('Mini Batch Iterations', fontsize=18)\n",
    "plt.title('Test Acc', fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(handles=[name_cost_main, name_supervised_costs, name_unsupervised_costs], fontsize=18)\n",
    "plt.savefig(f'ladder_mnist_loss_{int(labeled_ratio*100)}%_label_{initial_learning_rate:.3f}_lr_{int(unsupervised_costs_lambda[-1])}_lamda_{details}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "my_dict = {'accuracies':accuracies,'cost_main':cost_main,'supervised_costs':supervised_costs,\n",
    "           'unsupervised_costs':unsupervised_costs}\n",
    "df = pd.DataFrame(dict([ (k,Series(v)) for k,v in my_dict.items() ]))\n",
    "df.to_csv(f'ladder_mnist_{int(labeled_ratio*100)}%_label_{initial_learning_rate:.3f}_lr_{int(unsupervised_costs_lambda[-1])}_lamda_{details}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True \n",
    "\n",
    "epochs = 50\n",
    "decay_epoch = 15\n",
    "learning_rate = 0.02\n",
    "initial_learning_rate = learning_rate\n",
    "optimizer = Adam(just_encoder.parameters(), lr = learning_rate)\n",
    "loss_supervised = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "if use_cuda:\n",
    "    just_encoder.cuda()\n",
    "\n",
    "batch_size = 100\n",
    "batch_times = np.shape(X_train_labelled)[0]//batch_size\n",
    "\n",
    "supervised_costs = []\n",
    "y_pred_all = []\n",
    "\n",
    "agg_supervised_cost = 0.\n",
    "num_batches = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if epoch > decay_epoch:\n",
    "        ratio = float(epochs - epoch) / (epochs - decay_epoch)\n",
    "        learning_rate = learning_rate * ratio\n",
    "        optimizer = Adam(just_encoder.parameters(), lr = learning_rate)\n",
    "        \n",
    "    for i in range(batch_times):\n",
    "        \n",
    "        if (i+1)%100==0 or i==batch_times-1: print(f'epoch = {epoch}, batch = {i+1}')\n",
    "    \n",
    "        just_encoder.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_train_labelled_images = torch.FloatTensor(X_train_labelled[i*batch_size:(i+1)*batch_size-1])\n",
    "        batch_train_labelled_labels = torch.LongTensor(y_train[i*batch_size:(i+1)*batch_size-1])\n",
    "        \n",
    "        if use_cuda:\n",
    "            batch_train_labelled_images = batch_train_labelled_images.cuda()\n",
    "            batch_train_labelled_labels = batch_train_labelled_labels.cuda()\n",
    "        \n",
    "        labelled_data = Variable(batch_train_labelled_images, requires_grad=False)\n",
    "        labelled_target = Variable(batch_train_labelled_labels)\n",
    "        \n",
    "        # encoders\n",
    "        output_noise_labelled = just_encoder.forward_clean(labelled_data)      \n",
    "        \n",
    "        # costs\n",
    "        cost_supervised = loss_supervised.forward(output_noise_labelled, labelled_target)\n",
    "        \n",
    "        # backprop\n",
    "        cost_supervised.backward()                  \n",
    "        optimizer.step()\n",
    "    \n",
    "        agg_supervised_cost += cost_supervised.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        supervised_costs.append(cost_supervised.item())\n",
    "        \n",
    "    # eval\n",
    "    just_encoder.eval()\n",
    "    with torch.no_grad(): # So we don't run out of memory \n",
    "        if use_cuda:\n",
    "            X_test = X_test.cuda()\n",
    "        \n",
    "        X_test_cuda = Variable(X_test)\n",
    "        y_pred = just_encoder.forward_clean(X_test_cuda)\n",
    "        y_pred_all.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details_just_encoders = details + '_just_encoders'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,9))\n",
    "accuracies = []\n",
    "for i in y_pred_all:\n",
    "    pred_proba = i.data.cpu()\n",
    "    pred = np.argmax(pred_proba, 1)\n",
    "    accuracies.append(accuracy_score(y_test.numpy(),pred))\n",
    "plt.plot(np.arange(1,len(accuracies)+1),accuracies)\n",
    "plt.ylabel('Accuracy', fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.title('Test Acc (just encoder)', fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "notes = mpatches.Patch(color='#1F77B4', label=f'cost.back(), \\nini_lr = {initial_learning_rate:.3f}, \\\n",
    "last_lamda={unsupervised_costs_lambda[-1]} \\ndecay_ep={decay_epoch}, \\nlabelled={np.shape(X_train_labelled)[0]}({labeled_ratio}%)\\\n",
    "\\nunlabelled={np.shape(X_train_unlabelled)[0]}({1-labeled_ratio}%)\\\n",
    "\\ntest = {np.shape(X_test)[0]}')\n",
    "plt.legend(handles=[notes], fontsize=18)\n",
    "#plt.savefig(f'ladder_mnist_acc_{int(labeled_ratio*100)}%_label_{initial_learning_rate:.3f}_lr_{int(unsupervised_costs_lambda[-1])}_lamda_{details_just_encoders}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,9))\n",
    "name_supervised_costs, = plt.plot(np.arange(1,len(supervised_costs)+1), supervised_costs, label='supervised_cost')\n",
    "plt.ylabel('Loss', fontsize=18)\n",
    "plt.xlabel('Mini Batch Iterations', fontsize=18)\n",
    "plt.title('Test Acc', fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(handles=[name_supervised_costs], fontsize=18)\n",
    "#plt.savefig(f'ladder_mnist_loss_{int(labeled_ratio*100)}%_label_{initial_learning_rate:.3f}_lr_{int(unsupervised_costs_lambda[-1])}_lamda_{details_just_encoders}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'accuracies':accuracies,'supervised_costs':supervised_costs}\n",
    "df = pd.DataFrame(dict([ (k,Series(v)) for k,v in my_dict.items() ]))\n",
    "df.to_csv(f'ladder_mnist_{int(labeled_ratio*100)}%_label_{initial_learning_rate:.3f}_lr_{int(unsupervised_costs_lambda[-1])}_lamda_{details_just_encoders}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare ladder vs encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ladder_vs_encoder(df_ladder, df_encoder, train_samples):\n",
    "    plt.figure(figsize=(14,9))\n",
    "    name_ladder, = plt.plot(df_ladder['accuracies'].values, label = 'ladder')\n",
    "    name_encoder, = plt.plot(df_encoder['accuracies'].values, label = 'encoder')\n",
    "    plt.ylabel('Acc', fontsize=18)\n",
    "    plt.xlabel('Epochs', fontsize=18)\n",
    "    plt.title('Ladder vs Encoder (Test Acc)', fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    notes = mpatches.Patch(hatch='.',color='#FFFFFF', label=f'{train_samples} train samples')\n",
    "    plt.legend(handles=[name_ladder, name_encoder, notes], fontsize=18)\n",
    "    plt.ylim([.3, 1])\n",
    "    #plt.savefig(f'ladder_vs_encoder_{train_samples}_samples.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 1000\n",
    "results_path = 'ladder results - small subsets/'\n",
    "df_encoder = pd.read_csv(results_path + f'ladder_mnist_10%_label_0.020_lr_1000_lamda_{train_samples}_train_samples_just_encoders.csv')\n",
    "df_ladder = pd.read_csv(results_path + f'ladder_mnist_10%_label_0.020_lr_1000_lamda_{train_samples}_train_samples.csv')\n",
    "ladder_vs_encoder(df_ladder, df_encoder, train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 2000\n",
    "results_path = 'ladder results - small subsets/'\n",
    "df_encoder = pd.read_csv(results_path + f'ladder_mnist_10%_label_0.020_lr_1000_lamda_{train_samples}_train_samples_just_encoders.csv')\n",
    "df_ladder = pd.read_csv(results_path + f'ladder_mnist_10%_label_0.020_lr_1000_lamda_{train_samples}_train_samples.csv')\n",
    "ladder_vs_encoder(df_ladder, df_encoder, train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 4000\n",
    "results_path = 'ladder results - small subsets/'\n",
    "df_encoder = pd.read_csv(results_path + f'ladder_mnist_10%_label_0.020_lr_1000_lamda_{train_samples}_train_samples_just_encoders.csv')\n",
    "df_ladder = pd.read_csv(results_path + f'ladder_mnist_10%_label_0.020_lr_1000_lamda_{train_samples}_train_samples.csv')\n",
    "ladder_vs_encoder(df_ladder, df_encoder, train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 8000\n",
    "results_path = 'ladder results - small subsets/'\n",
    "df_encoder = pd.read_csv(results_path + f'ladder_mnist_10%_label_0.020_lr_1000_lamda_{train_samples}_train_samples_just_encoders.csv')\n",
    "df_ladder = pd.read_csv(results_path + f'ladder_mnist_10%_label_0.020_lr_1000_lamda_{train_samples}_train_samples.csv')\n",
    "ladder_vs_encoder(df_ladder, df_encoder, train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 16000\n",
    "results_path = 'ladder results - small subsets/'\n",
    "df_encoder = pd.read_csv(results_path + f'ladder_mnist_10%_label_0.020_lr_1000_lamda_{train_samples}_train_samples_just_encoders.csv')\n",
    "df_ladder = pd.read_csv(results_path + f'ladder_mnist_10%_label_0.020_lr_1000_lamda_{train_samples}_train_samples.csv')\n",
    "ladder_vs_encoder(df_ladder, df_encoder, train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 32000\n",
    "results_path = 'ladder results - small subsets/'\n",
    "df_encoder = pd.read_csv(results_path + f'ladder_mnist_10%_label_0.020_lr_1000_lamda_{train_samples}_train_samples_just_encoders.csv')\n",
    "df_ladder = pd.read_csv(results_path + f'ladder_mnist_10%_label_0.020_lr_1000_lamda_{train_samples}_train_samples.csv')\n",
    "ladder_vs_encoder(df_ladder, df_encoder, train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "481px",
    "left": "896.667px",
    "right": "20px",
    "top": "120px",
    "width": "348px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
